# Smooth Exact Gradient Descent Learning in Spiking Neural Networks

**Authors:** Christian Klos, Raoul-Martin Memmesheimer

**Presentation type:** Poster at [SNUFA 2023 online workshop (7-8 Nov 2023)](https://snufa.net/2023)

## Abstract

Artificial neural networks are highly successfully trained with backpropagation. For spiking neural networks, however, gradient descent learning is hindered by two fundamental problems: First, the vanishing and addition of spikes leads to disruptive, discontinuous changes in the spiking network dynamics, which are not taken into account by the gradient. Second, the gradient cannot systematically generate or remove spikes, even if required. These problems have so far been ignored or circumvented by using heuristics. 

Here we show how both problems may be solved. Specifically, we demonstrate exact gradient descent learning based on spiking dynamics that change continuously or even smoothly in the entire space of network parameters. Such dynamics are generated by neuron models whose spikes vanish and appear at the end of a trial, where they do not influence other neurons anymore. Among others, neuron models that generate spikes via a self-amplification mechanism and reach infinite voltage in finite time, such as the standard quadratic leaky integrate-and-fire (QIF) neuron, fulfill this condition. Besides spike removal, such neurons also enable gradient-based spike addition by means of what we call pseudospikes. The timings of pseudospikes are continuous and mostly smooth extensions of the times of ordinary spikes disappearing at the trial end.

We apply our scheme to individual and networks of QIF neurons using event-based simulations and automatic differentiation to compute spike-based gradients. Interestingly, this is possible even for QIF neurons with exponentially decaying input current given certain parameter combinations, for which we show how to derive the necessary analytical solutions. Using our scheme, we induce and continuously move spikes to desired times, in single neurons and recurrent networks. Further, we achieve competitive performance on MNIST using deep, initially silent networks and time-to-first-spike coding.

Taken together, our results show how non-disruptive, exact gradient descent learning is possible despite discrete spikes.